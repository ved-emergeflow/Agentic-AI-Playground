import os
from typing import TypedDict, Annotated, Literal, List, Any
from langchain.agents import create_agent
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain_core.messages import BaseMessage, SystemMessage, AIMessage, ToolMessage, HumanMessage
from langchain.tools import tool, ToolRuntime
from langchain_core.output_parsers import JsonOutputParser
from langgraph.prebuilt import ToolNode
from langgraph.types import Command, interrupt
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import add_messages
from langgraph.graph import StateGraph, START, END
from langchain_groq import ChatGroq
from langchain_community.tools import DuckDuckGoSearchRun
from pydantic import BaseModel, Field

# -------------------------------- State -------------------------------
"""
This is where Agent State is defined
messages -> List of BaseMessages (AIMessage, HumanMessage etc). 
add_messages is inbuilt reducer function to add messages to lost when state is updated.
user_input -> User input stored for generating insights later
steps -> Planned steps created by LLM
reasoning -> Reasoning for the same
changes -> Human added feedback to the research model
first_response, current_response, final_response -> stored to generate insights
jump_to -> Deprecated for this. Only use for Handoff based multi-agent systems
content-> List of research materials generated by llm and passed to the user during HITL interruption
insights -> Long term insights found out by the model after analyzing input, research responses and changes by the user.
first_run -> Flag to determine if insights are passed or this is fresh run.
approved -> To check if HITL response is approved( To skip HITL if approved)
counter -> Keep track of current reruns of a node (unused for now)
max_loop -> Hard max limit for loops (Unused for now)
"""
class RLAgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    user_input: str
    steps: str
    reasoning: str
    changes: list[str]
    first_response: str
    current_response: str
    final_response: str
    jump_to: str
    content: list[Any]
    insights: str
    first_run: bool
    approved: bool
    counter: int
    max_loop: int


# ----------------------------------------- Utils ----------------------------
MODEL = 'meta-llama/llama-4-maverick-17b-128e-instruct'

def get_groq_model(model):

    return ChatGroq(
        model=model,
        temperature=0.2,
        api_key=os.getenv('GROQ_API_KEY')
    )

model = get_groq_model(MODEL)

# ------------------------------------------ Tools -------------------------
wrapper = DuckDuckGoSearchAPIWrapper(region="in", time="d", max_results=2)
search_tool = DuckDuckGoSearchRun(api_wrapper=wrapper, source="news")


tools = [search_tool]

# ----------------------------------- Output Parsers ----------------------------

class Reasoning(BaseModel):
    steps: str = Field(description='All the planned steps in order', min_length=1)
    reasoning: str = Field(description='Reasoning and thought process', min_length=1)

class Research(BaseModel):
    target_age_group: Literal['18 and below','18-45','45 and above', 'All age groups']
    target_gender: Literal['Male','Female', 'Non-Binary', 'Everyone']
    target_persona: str = Field(description='Persona of people who might be interested in this', min_length=1)
    research_reasoning: str = Field(description='Reasoning for selecting the target demographics', min_length=1)

class Insights(BaseModel):
    insights: str = Field(description='Insights created')

reasoning_parser = JsonOutputParser(pydantic_object=Reasoning)
research_parser = JsonOutputParser(pydantic_object=Research)
insights_parser = JsonOutputParser(pydantic_object=Insights)

# ----------------------------------- Input Prompt Chains------------------------------

reasoning_prompt = ChatPromptTemplate.from_template(
    """
    You are given a user input {input}. 
    Your job is to plan your steps and mention your reasoning/ thought process behind your planning. 
    You have these tools: {tools},
    In your reasoning mention your indepth thought process and explanations as to why you think this is the correct approach.
    Do not use any tools.
    Return your response in JSON format with this structure:
    {format_instructions}
    """
)
reasoning_chain = reasoning_prompt | model.bind(response_format={'type':'json_object'}) | reasoning_parser

reasoning_with_insights_prompt = ChatPromptTemplate.from_template(
    """
    You are given a user input: {input} and insights from previous runs: {insights}. 
    Your job is to plan your steps for tackling the query and mention your reasoning or thought process behind your planning for each step. 
    You have these tools: {tools}.
    In your reasoning mention your indepth thought process and explanations as to why you think this is the correct approach.
    Return your response in JSON format with this structure:
    {format_instructions}
    """
)
reasoning_with_insights_chain = reasoning_with_insights_prompt | model.bind(response_format={'type':'json_object'}) | reasoning_parser

# -------------------------------------- SubAgents ----------------------------

search_agent = create_agent(model=model, tools=tools, state_schema=RLAgentState)

@tool
def search_browser(query : str | None, runtime: ToolRuntime[None | RLAgentState]) -> Command:
    """
    This tool calls another agent who search the web to find results
    :param query: str
    :return: Command
    """
    print('\nEntering search browser tool.....')
    messages = runtime.state['messages']
    # tool_call_id = state['messages'][-1].tool_calls[-1]['id']

    res = search_agent.invoke({
        'messages': [SystemMessage(content='You are an agent who can use search tools to find the most relevant texts based on the user queries from the web')] + messages,
    })

    # res = search_agent.invoke({
    #     'messages': SystemMessage(content='You are an agent who can use search tools to fnd the most relevant texts based on the user queries from the web') + HumanMessage(content=query),
    # })

    return Command(update=
        {
            'messages': [ToolMessage(content=res['messages'][-1].content, tool_call_id=runtime.tool_call_id)]
        }
    )

@tool()
def create_research(runtime: ToolRuntime[None | RLAgentState]) -> Command:
    """
    This tool calls another agent who creates a research based on all the gathered information from tools
    :return: Command
    """
    print('\nEntering Research tool.....')
    messages = runtime.state['messages']

    system_message = SystemMessage(content=f'You are an agent that conducts research based on the provided information from web search and responds in a structured way. Provide output in Json format using following instructions {research_parser.get_format_instructions()}')

    chain = model.bind(response_format={"type": "json_object"}) | research_parser

    res = chain.invoke([system_message]+messages)
    print(res)
    out_message = f"""
    The Research conducted is as follows:
    The Target Age group is {res['target_age_group']},
    The Target Gender is {res['target_gender']},
    The Target Persona who is interested is {res['target_persona']},
    The why: {res['research_reasoning']}
    """

    if runtime.state['first_run']:

        return Command(update=
            {
                'messages' : [ToolMessage(content=out_message, tool_call_id=runtime.tool_call_id)],
                'first_run': False,
                'first_response': out_message,
                'current_response': out_message,
                'content': [res['target_age_group'],res['target_gender'],res['target_persona'], res['research_reasoning']]
            }
        )

    else:
        return Command(update=
            {
                'messages' : [ToolMessage(content=out_message, tool_call_id=runtime.tool_call_id)],
                'current_response': out_message,
                'content': [res['target_age_group'],res['target_gender'],res['target_persona'], res['research_reasoning']]
            }
        )


# ----------------------------------------- Workflow Nodes ----------------------------

tool_agents = [search_browser, create_research]

def planning_reasoning_step(state: RLAgentState):
    print('\nEntering Planning Reasoning Node.......')
    if state['first_run']:
        print('Without Insights..')
        res = reasoning_chain.invoke(
            {
                'input' : state['user_input'],
                'tools' : tool_agents,
                'format_instructions': reasoning_parser.get_format_instructions()
            }
        )
        print('\n',res)
        return {
            'messages' : AIMessage(content='The Plan is as follows : ' + res['steps'] + '\n\n' +
                                           'The reasoning for them is as follows : ' + res['reasoning']),
            'reasoning' : res['reasoning'],
            'steps': res['steps'],
        }

    else:
        print('With Insights..')
        res = reasoning_with_insights_chain.invoke(
            {
                'input' : state['user_input'],
                'insights': state['insights'],
                'tools' : tool_agents,
                'format_instructions': reasoning_parser.get_format_instructions()
            }
        )
        return {
            'messages' : AIMessage(content='The Plan is as follows : ' + res['steps'] + '\n\n' +
                                           'The reasoning for them is as follows : ' + res['reasoning']),
            'reasoning' : res['reasoning'],
            'steps' : res['steps'],
        }


def router(state: RLAgentState):
    print('\nEntering Router .....')

    try:
        changes = state['changes']
        print('Router with user changes called....')
        messages = state['messages'] + [AIMessage(content=f'These changes have been asked by the User {changes}. Incorporate them and change your research')]

        # if len(messages) > 5:
        #     messages = messages[1:]

        res = model.bind_tools(tool_agents).invoke(messages)
        try:
            print(f'\nTool called: {res.tool_calls[0]['name']}')
        except:
            print('\nNo tool called')
        return {
            'messages' : res
        }
    except:

        messages = state['messages']
        # if len(messages) > 5:
        #     messages = messages[1:]

        res = model.bind_tools(tool_agents).invoke(messages)
        try:
            print(f'\nTool called: {res.tool_calls[0]['name']}')
        except:
            print('\nNo tool called')
        return {
            'messages' : res
        }

def which_agent(state: RLAgentState)->Literal['search_agent',END]:

    last_message = state['messages'][-1]
    if len(last_message.tool_calls)>0:
        tool = last_message.tool_calls[0]['name']
        if tool == 'search_browser':
            return 'search_agent'
        elif tool == 'create_research':
            return 'research_agent'
        else:
            return END

    else:
        return END

def hitl(state: RLAgentState):

    message = state['current_response'] if 'current_response' in state else state['messages'][-1].content
    if state['approved'] or 'approved' not in state:
        print('\nEntering HITL last time.....')
        return {
            'messages' : state['messages'],
        }

    else:
        print('\nEntering HITL.......')
        output = [state['reasoning'], message, state['content'] if 'content' in state else None]
        changes = interrupt(
            output
        )

        print('Human Feedback:  ', changes)
        if 'changes' not in state or state['changes'] is None:
            state['changes'] = [changes]
        if changes == 'Approved':
            return {
                'messages': AIMessage(content='No changes needed. Proceed to Finalise Research'),
                'final_response': state['current_response'],
                'jump_to': 'END',
                'approved': True
            }

        else:
            try:
                print('\n',state['changes'])
            except:
                print('\nNo changes exist')
            return {
                'messages': HumanMessage(content=f'Changes added. Refer the changes : {changes}. Redo the create research step'),
                'changes': state['changes'].append(changes),
                'jump_to': 'router',
                'approved': False
            }

def is_approved(state: RLAgentState)->Literal[END,'router']:

    if state['jump_to'] == 'END':
        return END
    else:
        return 'router'

def create_insights(state: RLAgentState):

    first_output = state['first_response'] if 'first_response' in state else state['messages'][-1]
    input = state['user_input']
    final_response = state['current_response'] if 'current_response' in state else state['messages'][-1].content
    changes = state['changes'] if 'changes' in state else ['No changes made by Human']

    prompt = f"""
    You are an agent which generates insights for long term knowledge gain based on user query, first and last agent responses and the human given changes.
    User Input : {input}
    First Response : {first_output}
    Last Response : {final_response}
    changes: {changes}
    Give Response in a JSON format using the following instructions {insights_parser.get_format_instructions()}
    """

    chain = model.bind(response_format={'type':'json_object'}) | insights_parser
    res = chain.invoke(prompt)

    return {
        'messages' : AIMessage(content=res['insights']),
        'insights' : res['insights'],
    }

search_browser_node = ToolNode(name='search_agent', tools=[search_browser])
create_research_node = ToolNode(name='research_agent', tools=[create_research])

# ---------------------------------------- Graph -----------------------------

graph = StateGraph(RLAgentState)

graph.add_node('planning_reasoning', planning_reasoning_step)
graph.add_node('router', router)
graph.add_node('search_browser', search_browser_node)
graph.add_node('create_research', create_research_node)
graph.add_node('hitl', hitl)
graph.add_node('insights', create_insights)
graph.add_edge(START, 'planning_reasoning')
graph.add_edge('planning_reasoning', 'router')
graph.add_conditional_edges('router', which_agent, {
    'search_agent': 'search_browser',
    'research_agent':'create_research',
    END : 'hitl'
})
graph.add_edge('search_browser','router')
graph.add_edge('create_research', 'hitl')
graph.add_conditional_edges('hitl', is_approved, {
    END: 'insights',
    'router': 'router'
})


# -------------------------------------- Agent Utils ------------------------------

def get_groq_agent(checkpointer):

    return graph.compile(checkpointer=checkpointer)
